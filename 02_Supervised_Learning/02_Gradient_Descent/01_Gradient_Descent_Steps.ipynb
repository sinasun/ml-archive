{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829b1533",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "In previous notes, we used gradient descent to solve regression for linear functions. Now we want to gather the step that works for any function, multi variable non-linear functions! The order is same as what we did for multi variable linear regression. But lets recap what we did.\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "The process to come up with a formula for our function is called feature engineering. This is where we use our logic to put the features we have, to come up with a function that should predict the output.\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "First we need to select the features that has impact on the output. For example, when predicting a house price, the unit number of house might not have any impact on the price of it (In some special case it might have!) but the number of bedrooms it has definitely has a impact on the price of house. \n",
    "\n",
    "So first we should select the related features to our output\n",
    "\n",
    "\n",
    "### Adding Features\n",
    "\n",
    "Sometimes that we have a better understandation of the output, we might be able to introduce a new feature that explain that data better. For example, for the house pricing example, we know that houses with 0 age, which called new homes, are more expensive than others. But after they are not new homes, the price drops down a lot. So we can add a new features that from age it decides if it's a new home or no (is_new_home: true or false). It's true that we already was using the age of houses to predict price, but now we have a better feature that can explain a big drop on the price. So outcome prediction is more clear now.\n",
    "\n",
    "\n",
    "### Final Feature Engineering Step\n",
    "\n",
    "Now that we have list of features that we know have impact on the output, we need to come up with a formula that can explain the relation between the features and the output. The more understandation from data we have, the better function we can come up with:\n",
    "$ f_{\\mathbf{w}}(\\mathbf{x^{(i)}}) $\n",
    "\n",
    "## Scaling\n",
    "\n",
    "You can refer to multi variable linear regression to see how we apply scaling on a function. The mathematical process is same for each function.\n",
    "\n",
    "### Normalization Scaling\n",
    "\n",
    "For feature **j**, we use this formula to scale each of training data:\n",
    "\n",
    "$$  x^{(i)}_{\\text{scaled},j} = \\frac{x^{(i)}_j - \\min{x_j}}{\\max{x_j} - \\min{x_j}} $$\n",
    "\n",
    "\n",
    "### Standardization Scaling \n",
    "\n",
    "There is a better way of scaling, that uses Z-score to scale features. First lets define some famous statistic function (**m** is number of data we have):\n",
    "\n",
    "**Population Mean**\n",
    "\n",
    "$$ \\mu = \\frac{\\sum_{i=0}^{m-1} x_i}{m} $$\n",
    "\n",
    "**Standard Deviation:**\n",
    "\n",
    "$$ \\sigma = \\sqrt{\\frac{\\sum_{i=0}^{m-1}(x_i - \\mu)^2}{m}} $$\n",
    "\n",
    "**X-Scaled**\n",
    "\n",
    "\n",
    "$$\\Rightarrow x_\\text{scaled,j}^{(i)} = \\frac{x^{(i)} - \\mu(x_j)}{\\sigma(x_j)} $$\n",
    "\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "Now that we have $ f_{\\mathbf{w}}(\\mathbf{x^{(i)}}) $ we can find the cost function:\n",
    "\n",
    "$$ J_{\\mathbf{w}} = \\frac{1}{2m} \\sum_{i=0}^{m-1}(f_{\\mathbf{w}}(\\mathbf{x^{(i)}}) - y^{(i)})^2$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7d385",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "###### It's going to be completed later!!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
