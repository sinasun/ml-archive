{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf93e23",
   "metadata": {},
   "source": [
    "# One Variable Linear Regression\n",
    "\n",
    "## Goal\n",
    "\n",
    "We're given a dataset that has x and y. Example:\n",
    "\n",
    "|  x1  | x2 | x3 | x4 |  y  |\n",
    "|------|----|----|----|-----|\n",
    "| 2104 | 5  | 1  | 45 | 460 |\n",
    "| 1416 | 3  | 2  | 40 | 232 |\n",
    "| 852  | 2  | 1  | 35 | 178 |\n",
    "\n",
    "\n",
    "Now we want to fit the best variable linear regression for the given data. Model (here n = 4):\n",
    "\n",
    "$$ f_{w,b}(x^{(i)}) = w_1 x_1^{(i)} + w_2 x_2^{(i)} + ... + w_n x_n^{(i)} + b $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4a8e6",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "We imagine we have **m** training data. Also, our function has **n** variables.\n",
    "To show all of x training data we use capital bold **X** which is a matrix:\n",
    "\n",
    "$$\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    " We also define a bold $\\mathbf{x^{(i)}}$ to show list of all training data for a training set and a bold **w**. \n",
    "\n",
    "$$\\mathbf{x}^{(i)} = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$$\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Also, we know that dot production of two vector is equal to:\n",
    "\n",
    "$$ \\mathbf{w} \\cdot \\mathbf{x^{(i)}} = \\sum_{j=0}^{n} (w_j x_j^{(i)}) $$\n",
    "\n",
    "And we can write our model as:\n",
    "\n",
    "$$ f_{\\mathbf{w}, b}(\\mathbf{x^{(i)}}) = \\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4072d0a9",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Now we know that our cost function is defined as below:\n",
    "\n",
    "$$ J_{\\mathbf{w}, b} = \\frac{1}{2m} \\sum_{i=0}^{m-1}(f_{\\mathbf{w}, b}(\\mathbf{x^{(i)}}) - y^{(i)})^2$$\n",
    "\n",
    "Now we need to calculate each of $ \\frac {\\partial J_{\\mathbf{w}, b}} {\\partial w_1} $ ,$ \\frac {\\partial J_{\\mathbf{w}, b}} {\\partial w_2} $, ... ,$ \\frac {\\partial J_{\\mathbf{w}, b}} {\\partial w_n} $, $ \\frac {\\partial J_{\\mathbf{w}, b}} {\\partial b} $. \n",
    "\n",
    "$$ \\frac {\\partial J_{\\mathbf{w}, b}} {\\partial w_1} = \\frac{\\partial}{\\partial w_1} ( \\frac{1}{2m} \\sum_{i=0}^{m-1}(f_{\\mathbf{w}, b}(\\mathbf{x^{(i)}}) - y^{(i)})^2)$$\n",
    "\n",
    "$$ \\frac {\\partial J_{\\mathbf{w}, b}} {\\partial w_1} = \\frac{\\partial}{\\partial w_1} ( \\frac{1}{2m} \\sum_{i=0}^{m-1}(w_1 x_1 ^{(i)} + w_2 x_2^{(i)} + ... + w_n x_n^{(i)} - y^{(i)})^2)$$\n",
    "\n",
    "\n",
    "$$ \\frac {\\partial J_{\\mathbf{w}, b}} {\\partial w_1} = \\frac{1}{m} \\sum_{i=0}^{m-1}(w_1 x_1^{(i)} + w_2 x_2^{(i)} + ... + w_n x_n^{(i)} - y^{(i)}) \\times x_1 ^ {(i)}$$\n",
    "\n",
    "By replacing $f_{w,b}$ we get:\n",
    "\n",
    "$$\\frac {\\partial J_{\\mathbf{w}, b}} {\\partial w_1} = \\frac{1}{m} \\sum_{i=0}^{m-1}(f_{\\mathbf{w}, b}(\\mathbf{x^{(i)}}) - y^{(i)}) \\times x_1 ^ {(i)}$$\n",
    "\n",
    "And we can generalize it so we get:\n",
    "\n",
    "\n",
    "$$\\Rightarrow \\frac {\\partial J_{\\mathbf{w}, b}} {\\partial w_n} = \\frac{1}{m} \\sum_{i=0}^{m-1}(f_{\\mathbf{w}, b}(\\mathbf{x^{(i)}}) - y^{(i)}) \\times x_n ^ {(i)}$$\n",
    "\n",
    "By repeating same steps for b, we get:\n",
    "\n",
    "\n",
    "$$\\Rightarrow \\frac {\\partial J_{\\mathbf{w}, b}} {\\partial b} = \\frac{1}{m} \\sum_{i=0}^{m-1}(f_{\\mathbf{w}, b}(\\mathbf{x^{(i)}}) - y^{(i)})$$\n",
    "\n",
    "And from one linear regression, we are familiar with these formulas:\n",
    "\n",
    "$$ w_n = w_n - \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_n} $$\n",
    "$$ b = b - \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e7cd81",
   "metadata": {},
   "source": [
    "## Code\n",
    "### Initial Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597b150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db2e407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X_train, y_train, w, b):\n",
    "    m = X_train.shape[0]\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        error = np.dot(w, X_train[i]) + b - y_train[i]\n",
    "        cost += error ** 2\n",
    "    cost = cost / (2 * m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "614fa62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X_train, y_train, w, b):\n",
    "    m = X_train.shape[0]\n",
    "    dj_dw = np.zeros(X_train.shape[1])\n",
    "    dj_db = 0.\n",
    "    for i in range(m):\n",
    "        error = np.dot(w, X_train[i]) + b - y_train[i]\n",
    "        dj_dw += np.dot(error, X_train[i])\n",
    "        dj_db += error\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "725011e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_regression(X_train, y_train, initial_w, initial_b, alpha, iteration):\n",
    "    w = initial_w\n",
    "    b = initial_b\n",
    "    for _ in range(iteration):\n",
    "        dj_dw, dj_db = compute_gradient(X_train, y_train, w, b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "990773ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 686.7034116665205\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "\n",
    "b_initial = 0.\n",
    "w_initial = np.zeros(X_train.shape[1])\n",
    "\n",
    "alpha = 5.0e-7\n",
    "iteration = 1_000\n",
    "w, b = fit_regression(X_train, y_train, w_initial, b_initial, alpha, iteration)\n",
    "\n",
    "cost = compute_cost(X_train, y_train, w, b)\n",
    "print(f\"cost: {cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47728ca8",
   "metadata": {},
   "source": [
    "### Applying Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef2f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
